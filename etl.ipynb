{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote_plus\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection(hostname, username, password, dbname):\n",
    "    # Initialize cinnection to None\n",
    "    con = None\n",
    "\n",
    "    # Encode the password\n",
    "    password = quote_plus(password)\n",
    "\n",
    "    # Create connection while checking for any errors\n",
    "    try:\n",
    "        con = mysql.connector.connect(host=hostname, user=username, passwd = password, database  = dbname)\n",
    "        print(\"Connection successful!\")\n",
    "    except Error as e:\n",
    "        print(f\"The error {e} has occured.\")\n",
    "    \n",
    "    return con\n",
    "\n",
    "def engine(hostname, username, password, dbname, port):\n",
    "    # Create engine\n",
    "    eng = create_engine(f'mysql+pymysql://{username}:{password}@{hostname}:{port}/{dbname}')\n",
    "    return eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_staging(datafile):\n",
    "    # Create engine using the pre defined engine function\n",
    "    eng = engine(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_source\", \"3306\")\n",
    "\n",
    "    # Load csv to dataframe\n",
    "    df = pd.read_csv(datafile, sep=', ', engine='python')\n",
    "\n",
    "    # Load dataframe to staging schema\n",
    "    df.to_sql(name=\"feedback\",con=eng, schema=\"feedback_source\",if_exists=\"replace\",index = False)\n",
    "\n",
    "csv_to_staging(\"Data/sentiment-analysis.csv\")\n",
    "\n",
    "def extract_source():\n",
    "    # Create connection and cursor\n",
    "    conn = connection(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_source\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # The query extracts all data in the staging schema\n",
    "    query = \"Select * from feedback_source.feedback\"\n",
    "    cursor.execute(query)\n",
    "    data = cursor.fetchall()\n",
    "\n",
    "    # Define columns names and load cursor data to a dataframe\n",
    "    column_names = [i[0] for i in cursor.description]\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_feedback():\n",
    "    # Extract the data from staging area\n",
    "    df = extract_source()\n",
    "\n",
    "    # Take a copy of the df\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop dupicates\n",
    "    df =  df.drop_duplicates()\n",
    "\n",
    "    # Rename columns properly\n",
    "    df.rename(columns={'\"Text': 'Text', 'Confidence Score\"': 'Confidence Score'}, inplace=True)\n",
    "\n",
    "    # Remove characters from values\n",
    "    df.loc[:,'Text'] = df['Text'].str.replace(r'\\\"', '', regex=True)\n",
    "    df.loc[:,'Confidence Score'] = df['Confidence Score'].str.replace(r'\\\"', '', regex=True)\n",
    "    df.loc[:,'User ID'] = df['User ID'].str.replace(r'@', '', regex=True)\n",
    "\n",
    "    # Change some data types\n",
    "    df['Date/Time'] = pd.to_datetime(df['Date/Time'])\n",
    "    df['Confidence Score'] = df['Confidence Score'].astype(dtype='float')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "def load_sources():\n",
    "    # Load transformd data into a dataframe\n",
    "    df = transform_feedback()\n",
    "\n",
    "    # Create engine\n",
    "    eng = engine(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_dwh\", \"3306\")\n",
    "\n",
    "    # Load distinct sources to a dataframe\n",
    "    sources = df[['Source']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Generate ids for the sources\n",
    "    sources['id'] = range(1, len(sources)+1)\n",
    "\n",
    "    # Organize the columns\n",
    "    sources = sources[['id', 'Source']]\n",
    "\n",
    "    # Load the sources to the corresponding table in the data warehouse\n",
    "    sources.to_sql(name=\"dim_source\", con=eng, schema=\"feedback_dwh\", if_exists=\"append\", index=False)\n",
    "\n",
    "    #return\n",
    "load_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "def load_locations():\n",
    "    # Load transformd data into a dataframe\n",
    "    df = transform_feedback()\n",
    "\n",
    "    # Create engine\n",
    "    eng = engine(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_dwh\", \"3306\")\n",
    "\n",
    "    # Load distinct locations to a dataframe\n",
    "    locations = df[['Location']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Generate ids for the locations\n",
    "    locations['id'] = range(1, len(locations)+1)\n",
    "\n",
    "    # Organize the columns\n",
    "    locations = locations[['id', 'Location']]\n",
    "\n",
    "    # Load the locations to the corresponding table in the data warehouse\n",
    "    locations.to_sql(name=\"dim_location\", con=eng, schema=\"feedback_dwh\", if_exists=\"append\", index=False)\n",
    "\n",
    "    return\n",
    "load_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "def load_sentiments():\n",
    "    # Load transformd data into a dataframe\n",
    "    df = transform_feedback()\n",
    "\n",
    "    # Create engine\n",
    "    eng = engine(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_dwh\", \"3306\")\n",
    "\n",
    "    # Load distinct locations to a dataframe\n",
    "    sentiments = df[['Sentiment']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Generate ids for the locations\n",
    "    sentiments['id'] = range(1, len(sentiments)+1)\n",
    "\n",
    "    # Organize the columns\n",
    "    sentiments = sentiments[['id', 'Sentiment']]\n",
    "\n",
    "    # Load the locations to the corresponding table in the data warehouse\n",
    "    sentiments.to_sql(name=\"dim_sentiment\", con=eng, schema=\"feedback_dwh\", if_exists=\"append\", index=False)\n",
    "\n",
    "    return\n",
    "load_sentiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
