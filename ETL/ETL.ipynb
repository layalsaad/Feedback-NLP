{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote_plus\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection(hostname, username, password, dbname):\n",
    "    # Initialize cinnection to None\n",
    "    con = None\n",
    "\n",
    "    # Encode the password\n",
    "    password = quote_plus(password)\n",
    "\n",
    "    # Create connection while checking for any errors\n",
    "    try:\n",
    "        con = mysql.connector.connect(host=hostname, user=username, passwd = password, database  = dbname)\n",
    "        print(\"Connection successful!\")\n",
    "    except Error as e:\n",
    "        print(f\"The error {e} has occured.\")\n",
    "    \n",
    "    return con\n",
    "\n",
    "def engine(hostname, username, password, dbname, port):\n",
    "    # Create engine\n",
    "    eng = create_engine(f'mysql+pymysql://{username}:{password}@{hostname}:{port}/{dbname}')\n",
    "    return eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "def csv_to_staging(datafile):\n",
    "    # Create engine using the pre defined engine function\n",
    "    eng = engine(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_source\", \"3306\")\n",
    "\n",
    "    # Load csv to dataframe\n",
    "    df = pd.read_csv(datafile, sep=', ', engine='python')\n",
    "\n",
    "    # Load dataframe to staging schema\n",
    "    df.to_sql(name=\"feedback\",con=eng, schema=\"feedback_source\",if_exists=\"replace\",index = False)\n",
    "\n",
    "csv_to_staging(\"../Data/sentiment-analysis.csv\")\n",
    "\n",
    "def extract_source():\n",
    "    # Create connection and cursor\n",
    "    conn = connection(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_source\")\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # The query extracts all data in the staging schema\n",
    "    query = \"Select * from feedback_source.feedback\"\n",
    "    cursor.execute(query)\n",
    "    data = cursor.fetchall()\n",
    "\n",
    "    # Define columns names and load cursor data to a dataframe\n",
    "    column_names = [i[0] for i in cursor.description]\n",
    "    df = pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "    # Extract to a csv file\n",
    "    df.to_csv(\"../Data/Extracted.csv\", index=False)\n",
    "\n",
    "extract_source()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_feedback():\n",
    "    # Extract the data from staging area\n",
    "    df = pd.read_csv(\"../Data/Extracted.csv\")\n",
    "\n",
    "    # Take a copy of the df\n",
    "    df = df.copy()\n",
    "\n",
    "    # Drop dupicates\n",
    "    df =  df.drop_duplicates()\n",
    "\n",
    "    # Rename columns properly\n",
    "    df.rename(columns={'\"Text': 'Text', 'Confidence Score\"': 'Confidence Score'}, inplace=True)\n",
    "\n",
    "    # Remove characters from values\n",
    "    df.loc[:,'Text'] = df['Text'].str.replace(r'\\\"', '', regex=True)\n",
    "    df.loc[:,'Confidence Score'] = df['Confidence Score'].str.replace(r'\\\"', '', regex=True)\n",
    "    df.loc[:,'User ID'] = df['User ID'].str.replace(r'@', '', regex=True)\n",
    "\n",
    "    # Change some data types\n",
    "    df['Date/Time'] = pd.to_datetime(df['Date/Time'])\n",
    "    df['Confidence Score'] = df['Confidence Score'].astype(dtype='float')\n",
    "\n",
    "    # Save transformed data in a csv\n",
    "    df.to_csv(\"../Data/Transformed.csv\", index=False)\n",
    "\n",
    "transform_feedback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sources():\n",
    "    # Load transformd data into a dataframe\n",
    "    df = pd.read_csv(\"../Data/Transformed.csv\")\n",
    "\n",
    "    # Create engine\n",
    "    eng = engine(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_dwh\", \"3306\")\n",
    "\n",
    "    # SQL commands to drop and create foreign key, create priary key and set its type\n",
    "    drop_fk = text(\"ALTER TABLE fact_feedback DROP FOREIGN KEY source_id;\")\n",
    "    add_fk = text(\"\"\"\n",
    "    ALTER TABLE fact_feedback\n",
    "    ADD CONSTRAINT source_id\n",
    "    FOREIGN KEY (source_id) REFERENCES dim_source(id);\n",
    "    \"\"\")\n",
    "    id_type = text(\"ALTER TABLE dim_source MODIFY COLUMN id INT;\")\n",
    "    add_pk = text(\"ALTER TABLE dim_source ADD PRIMARY KEY (id);\")\n",
    "\n",
    "    # Load distinct sources to a dataframe\n",
    "    sources = df[['Source']].drop_duplicates()\n",
    "\n",
    "    # Generate ids for the sources\n",
    "    sources = sources.reset_index(drop=True)\n",
    "    sources['id'] = sources.index + 1 \n",
    "\n",
    "    # Organize the columns\n",
    "    sources = sources[['id', 'Source']]\n",
    "\n",
    "    with eng.connect() as connection:\n",
    "        # Drop foreign key\n",
    "        connection.execute(drop_fk)\n",
    "\n",
    "        # Load the sources to the corresponding table in the data warehouse\n",
    "        sources.to_sql(name=\"dim_source\", con=eng, schema=\"feedback_dwh\", if_exists=\"replace\", index=False)\n",
    "\n",
    "        # Add primary key and set its type\n",
    "        connection.execute(add_pk)\n",
    "        connection.execute(id_type)\n",
    "\n",
    "        # Add foreign key\n",
    "        connection.execute(add_fk)\n",
    "\n",
    "    # Save sources into a csv\n",
    "    sources.to_csv(\"../Data/sources.csv\", index=False)\n",
    "\n",
    "load_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_locations():\n",
    "    # Load transformd data into a dataframe\n",
    "    df = pd.read_csv(\"../Data/Transformed.csv\")\n",
    "    # Create engine\n",
    "    eng = engine(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_dwh\", \"3306\")\n",
    "\n",
    "    # SQL commands to drop and create foreign key, create priary key and set its type\n",
    "    drop_fk = text(\"ALTER TABLE fact_feedback DROP FOREIGN KEY location_id;\")\n",
    "    add_fk = text(\"\"\"\n",
    "    ALTER TABLE fact_feedback\n",
    "    ADD CONSTRAINT location_id\n",
    "    FOREIGN KEY (location_id) REFERENCES dim_location(id);\n",
    "    \"\"\")\n",
    "    id_type = text(\"ALTER TABLE dim_location MODIFY COLUMN id INT;\")\n",
    "    add_pk = text(\"ALTER TABLE dim_location ADD PRIMARY KEY (id);\")\n",
    "\n",
    "    # Load distinct locations to a dataframe\n",
    "    locations = df[['Location']].drop_duplicates()\n",
    "\n",
    "    # Generate ids for the locations\n",
    "    locations = locations.reset_index(drop=True)\n",
    "    locations['id'] = locations.index + 1\n",
    "\n",
    "    # Organize the columns\n",
    "    locations = locations[['id', 'Location']]\n",
    "\n",
    "    with eng.connect() as connection:\n",
    "        # Drop foreign key\n",
    "        connection.execute(drop_fk)\n",
    "\n",
    "        # Load the locations to the corresponding table in the data warehouse\n",
    "        locations.to_sql(name=\"dim_location\", con=eng, schema=\"feedback_dwh\", if_exists=\"replace\", index=False)\n",
    "\n",
    "        # Add primary key and set its type\n",
    "        connection.execute(add_pk)\n",
    "        connection.execute(id_type)\n",
    "\n",
    "        # Add foreign key\n",
    "        connection.execute(add_fk)\n",
    "\n",
    "    # Save sources into a csv\n",
    "    locations.to_csv(\"../Data/locations.csv\", index=False)\n",
    "\n",
    "load_locations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentiments():\n",
    "    # Load transformd data into a dataframe\n",
    "    df = pd.read_csv(\"../Data/Transformed.csv\")\n",
    "\n",
    "    # Create engine\n",
    "    eng = engine(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_dwh\", \"3306\")\n",
    "\n",
    "    # SQL commands to drop and create foreign key, create priary key and set its type\n",
    "    drop_fk = text(\"ALTER TABLE fact_feedback DROP FOREIGN KEY sentiment_id;\")\n",
    "    add_fk = text(\"\"\"\n",
    "    ALTER TABLE fact_feedback\n",
    "    ADD CONSTRAINT sentiment_id\n",
    "    FOREIGN KEY (sentiment_id) REFERENCES dim_sentiment(id);\n",
    "    \"\"\")\n",
    "    id_type = text(\"ALTER TABLE dim_sentiment MODIFY COLUMN id INT;\")\n",
    "    add_pk = text(\"ALTER TABLE dim_sentiment ADD PRIMARY KEY (id);\")\n",
    "\n",
    "    # Load distinct locations to a dataframe\n",
    "    sentiments = df[['Sentiment']].drop_duplicates()\n",
    "\n",
    "    # Generate ids for the locations\n",
    "    sentiments = sentiments.reset_index(drop=True)\n",
    "    sentiments['id'] = sentiments.index + 1\n",
    "\n",
    "    # Organize the columns\n",
    "    sentiments = sentiments[['id', 'Sentiment']]\n",
    "\n",
    "    with eng.connect() as connection:\n",
    "        # Drop foreign key\n",
    "        connection.execute(drop_fk)\n",
    "\n",
    "        # Load the locations to the corresponding table in the data warehouse\n",
    "        sentiments.to_sql(name=\"dim_sentiment\", con=eng, schema=\"feedback_dwh\", if_exists=\"replace\", index=False)\n",
    "\n",
    "        # Add primary key and set its type\n",
    "        connection.execute(add_pk)\n",
    "        connection.execute(id_type)\n",
    "\n",
    "        # Add foreign key\n",
    "        connection.execute(add_fk)\n",
    "\n",
    "    # Save sources into a csv\n",
    "    sentiments.to_csv(\"../Data/sentiments.csv\", index=False)\n",
    "\n",
    "load_sentiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feedback():\n",
    "    # Load transformd data into a dataframe\n",
    "    df = pd.read_csv(\"../Data/Transformed.csv\")\n",
    "\n",
    "    # Create engine\n",
    "    eng = engine(\"localhost\", \"root\", \"Layaldbroot1997\", \"feedback_dwh\", \"3306\")\n",
    "\n",
    "    # Get dimension tables\n",
    "    df_locations = pd.read_csv(\"../Data/locations.csv\")\n",
    "    df_locations.rename(columns={'id':'location_id'},inplace=True)\n",
    "    df_sources = pd.read_csv(\"../Data/sources.csv\")\n",
    "    df_sources.rename(columns={'id':'source_id'},inplace=True)\n",
    "    df_sentiments = pd.read_csv(\"../Data/sentiments.csv\")\n",
    "    df_sentiments.rename(columns={'id':'sentiment_id'},inplace=True)\n",
    "\n",
    "    # Merge tables\n",
    "    merged = pd.merge(df, df_locations,on=\"Location\", how='left')\n",
    "    merged = pd.merge(merged, df_sentiments, on='Sentiment', how='left')\n",
    "    merged = pd.merge(merged, df_sources, on='Source', how='left')\n",
    "\n",
    "    # Generate ids for the feedback\n",
    "    merged = merged.reset_index(drop=True)\n",
    "    merged['id'] = merged.index + 1\n",
    "\n",
    "    # Drop unneeded columns and order columns\n",
    "    merged.drop(columns=['Sentiment', 'Source', 'Location'], inplace=True)\n",
    "    merged = merged[['id','Text','sentiment_id', 'Date/Time', 'User ID', 'location_id', 'source_id', 'Confidence Score']]\n",
    "\n",
    "    # Load the locations to the corresponding table in the data warehouse\n",
    "    merged.to_sql(name=\"fact_feedback\", con=eng, schema=\"feedback_dwh\", if_exists=\"replace\", index=False)\n",
    "\n",
    "    # Save feedback into a csv\n",
    "    merged.to_csv(\"../Data/feedback.csv\", index=False)\n",
    "\n",
    "load_feedback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
